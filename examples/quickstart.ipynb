{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Toolkit - Quick Start Guide\n",
    "\n",
    "Welcome! This notebook will get you from zero to productive in 15 minutes.\n",
    "\n",
    "We'll work through a complete example: **predicting customer churn** for a telecommunications company. You'll learn how to:\n",
    "\n",
    "- Quickly analyze your data\n",
    "- Clean and prepare features\n",
    "- Engineer new features\n",
    "- Select the most important features\n",
    "- Get insights and recommendations\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the package:\n",
    "\n",
    "```bash\n",
    "pip install feature-engineering-tk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from feature_engineering_tk import DataAnalyzer, TargetAnalyzer, DataPreprocessor, FeatureEngineer, FeatureSelector\n",
    "from feature_engineering_tk.data_analysis import quick_analysis\n",
    "from feature_engineering_tk.feature_selection import select_features_auto\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Customer Churn Dataset\n",
    "\n",
    "Let's create a realistic telecom customer dataset with common data issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate customer data\n",
    "n_customers = 2000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'age': np.random.randint(18, 80, n_customers),\n",
    "    'tenure_months': np.random.randint(1, 72, n_customers),\n",
    "    'monthly_charges': np.random.uniform(20, 120, n_customers),\n",
    "    'total_charges': np.random.uniform(100, 8000, n_customers),\n",
    "    'contract_type': np.random.choice(['Month-to-Month', 'One Year', 'Two Year'], n_customers, p=[0.5, 0.3, 0.2]),\n",
    "    'payment_method': np.random.choice(['Electronic Check', 'Credit Card', 'Bank Transfer', 'Mailed Check'], n_customers),\n",
    "    'internet_service': np.random.choice(['DSL', 'Fiber Optic', 'No'], n_customers, p=[0.3, 0.5, 0.2]),\n",
    "    'tech_support': np.random.choice(['Yes', 'No', 'No Internet'], n_customers, p=[0.3, 0.5, 0.2]),\n",
    "    'num_support_calls': np.random.poisson(2, n_customers),\n",
    "    'churn': np.random.choice([0, 1], n_customers, p=[0.73, 0.27])  # 27% churn rate\n",
    "})\n",
    "\n",
    "# Add some missing values (realistic scenario)\n",
    "missing_idx = np.random.choice(df.index, size=int(0.05 * n_customers), replace=False)\n",
    "df.loc[missing_idx, 'total_charges'] = np.nan\n",
    "\n",
    "missing_idx2 = np.random.choice(df.index, size=int(0.03 * n_customers), replace=False)\n",
    "df.loc[missing_idx2, 'tech_support'] = np.nan\n",
    "\n",
    "# Add some outliers\n",
    "outlier_idx = np.random.choice(df.index, size=20, replace=False)\n",
    "df.loc[outlier_idx, 'monthly_charges'] = np.random.uniform(200, 500, 20)\n",
    "\n",
    "# Add duplicate rows\n",
    "df = pd.concat([df, df.sample(5)], ignore_index=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick EDA: Instant Insights\n",
    "\n",
    "The `quick_analysis()` function gives you an immediate, comprehensive overview of your dataset in one line of code. It provides:\n",
    "\n",
    "- **Dataset Overview**: Shape, memory usage, and data types\n",
    "- **Missing Values**: Complete analysis of null values across all columns\n",
    "- **Numeric Features**: Statistical summaries (mean, std, min, max, quartiles)\n",
    "- **Categorical Features**: Unique value counts and cardinality analysis\n",
    "- **Outlier Detection**: Identifies potential outliers using IQR and Z-score methods\n",
    "- **Correlation Analysis**: Highlights strongly correlated features (Pearson method)\n",
    "- **Misclassified Categorical Detection**: Finds numeric columns that should be categorical (e.g., binary flags, low cardinality IDs)\n",
    "- **Binning Suggestions**: Recommends binning strategies for continuous features based on distribution characteristics\n",
    "\n",
    "This is your go-to function for the initial 30-second assessment of any new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Let's clean our data using DataPreprocessor. We'll use method chaining for efficiency.\n",
    "\n",
    "In most cases you will not know straight away what the issues with your data are, it will usually take some experimentation and time. Luckily in our case the issues were manufactured so we can get right to removing them, namly handling nulls, duplicates, and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor(df)\n",
    "\n",
    "# Chain multiple preprocessing steps\n",
    "preprocessor\\\n",
    "    .drop_columns(['customer_id'], inplace=True)\\\n",
    "    .remove_duplicates(inplace=True)\\\n",
    "    .handle_missing_values(strategy='median', columns=['total_charges'], inplace=True)\\\n",
    "    .handle_missing_values(strategy='mode', columns=['tech_support'], inplace=True)\\\n",
    "    .handle_outliers(columns=['monthly_charges'], method='iqr', action='cap', inplace=True)\n",
    "\n",
    "# Get cleaned data\n",
    "df_clean = preprocessor.get_dataframe()\n",
    "\n",
    "print(f\"\\nCleaned dataset shape: {df_clean.shape}\")\n",
    "print(f\"Missing values remaining: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# View preprocessing summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(preprocessor.get_preprocessing_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's understand our target variable and check for correlations.\n",
    "\n",
    "Some of the information we got in our quick analysis. We can also improve upon it by looking at the data from the prospective of the target as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlations\n",
    "analyzer = DataAnalyzer(df_clean)\n",
    "high_corr = analyzer.get_high_correlations(threshold=0.7)\n",
    "\n",
    "if not high_corr.empty:\n",
    "    print(\"High correlations found:\")\n",
    "    print(high_corr)\n",
    "else:\n",
    "    print(\"No high correlations (>0.7) found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable\n",
    "target_analyzer = TargetAnalyzer(df_clean, target_column='churn')\n",
    "\n",
    "# Check class distribution\n",
    "class_dist = target_analyzer.analyze_class_distribution()\n",
    "print(\"Class Distribution:\")\n",
    "print(class_dist)\n",
    "\n",
    "# Check for imbalance\n",
    "imbalance_info = target_analyzer.get_class_imbalance_info()\n",
    "print(f\"\\nClass Imbalance Severity: {imbalance_info['severity']}\")\n",
    "print(f\"Imbalance Ratio: {imbalance_info['imbalance_ratio']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Create new features that might help predict churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "engineer = FeatureEngineer(df_clean)\n",
    "\n",
    "# Encode categorical variables\n",
    "engineer.encode_categorical_onehot(\n",
    "    columns=['contract_type', 'payment_method', 'internet_service', 'tech_support'],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Create useful derived features\n",
    "engineer.create_ratio_features(\n",
    "    numerator='total_charges',\n",
    "    denominator='tenure_months',\n",
    "    name='avg_charges_per_month',\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Create a flag for high support calls\n",
    "engineer.create_flag_features(\n",
    "    column='num_support_calls',\n",
    "    condition=lambda x: x > 3,\n",
    "    flag_name='high_support_calls',\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Scale numeric features\n",
    "numeric_cols = ['age', 'tenure_months', 'monthly_charges', 'total_charges', 'num_support_calls', 'avg_charges_per_month']\n",
    "engineer.scale_features(\n",
    "    columns=numeric_cols,\n",
    "    method='standard',\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Get engineered dataframe\n",
    "df_engineered = engineer.get_dataframe()\n",
    "\n",
    "print(f\"Feature engineering complete. New shape: {df_engineered.shape}\")\n",
    "print(f\"\\nNew columns created: {df_engineered.shape[1] - df_clean.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Automatically select the most important features for predicting churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature selector and use automatic selection\n",
    "df_selected = select_features_auto(\n",
    "    df=df_engineered,\n",
    "    target_column='churn',\n",
    "    task='classification',\n",
    "    max_features=15,\n",
    "    variance_threshold=0.01,\n",
    "    correlation_threshold=0.95\n",
    ")\n",
    "\n",
    "print(f\"Original features: {df_engineered.shape[1] - 1}\")\n",
    "print(f\"Selected features: {df_selected.shape[1] - 1}\")\n",
    "print(f\"\\nFeatures removed: {df_engineered.shape[1] - df_selected.shape[1]}\")\n",
    "\n",
    "print(\"\\nSelected features:\")\n",
    "selected_features = [col for col in df_selected.columns if col != 'churn']\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    print(f\"{i}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis & Insights\n",
    "\n",
    "Get insights about feature-target relationships and model recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature-target relationships\n",
    "target_analyzer_final = TargetAnalyzer(df_selected, target_column='churn')\n",
    "\n",
    "relationships = target_analyzer_final.analyze_feature_target_relationship()\n",
    "relationships\n",
    "\n",
    "print(\"Top 10 Most Significant Features:\")\n",
    "print(relationships.head(10)[['feature', 'test_type', 'statistic', 'pvalue']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model recommendations\n",
    "model_recs = target_analyzer_final.recommend_models()\n",
    "\n",
    "print(\"Model Recommendations:\")\n",
    "for i, rec in enumerate(model_recs[:5], 1):\n",
    "    print(f\"\\n{i}. {rec['model']} (Priority: {rec['priority']})\")\n",
    "    print(f\"   Reason: {rec['reason']}\")\n",
    "    print(f\"   Note: {rec['considerations']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature engineering suggestions\n",
    "suggestions = target_analyzer_final.suggest_feature_engineering()\n",
    "\n",
    "print(\"Feature Engineering Suggestions:\")\n",
    "high_priority = [s for s in suggestions if s['priority'] == 'high']\n",
    "for i, sugg in enumerate(high_priority[:5], 1):\n",
    "    print(f\"\\n{i}. {sugg['feature']}\")\n",
    "    print(f\"   Suggestion: {sugg['suggestion']}\")\n",
    "    print(f\"   Reason: {sugg['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Report\n",
    "\n",
    "Save a comprehensive analysis report for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export report\n",
    "target_analyzer_final.export_report('churn_analysis_report.html', format='html')\n",
    "print(\"Report exported to churn_analysis_report.html\")\n",
    "\n",
    "# Also export preprocessing summary\n",
    "preprocessor.export_summary('preprocessing_report.md', format='markdown')\n",
    "print(\"Preprocessing report exported to preprocessing_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In just a few minutes, we've:\n",
    "\n",
    "âœ… **Cleaned the data**: Handled missing values, outliers, and duplicates  \n",
    "âœ… **Explored the data**: Found patterns and correlations  \n",
    "âœ… **Engineered features**: Created meaningful derived features  \n",
    "âœ… **Selected features**: Identified the most important predictors  \n",
    "âœ… **Generated insights**: Got model recommendations and feature suggestions  \n",
    "âœ… **Exported reports**: Saved comprehensive documentation  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Check out the **In-Depth Tutorial** (`tutorial_indepth.ipynb`) for advanced techniques\n",
    "- Apply these patterns to your own datasets\n",
    "- Explore statistical robustness features for production use\n",
    "- Save and load transformers for deployment\n",
    "\n",
    "Happy feature engineering! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
