{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Toolkit - In-Depth Tutorial\n",
    "\n",
    "This comprehensive tutorial covers advanced workflows and techniques for data preparation and feature engineering.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Exploratory Data Analysis** - Deep dive into DataAnalyzer and TargetAnalyzer\n",
    "2. **Data Preprocessing** - Handle messy real-world data\n",
    "3. **Feature Engineering** - Create powerful predictive features\n",
    "4. **Feature Selection** - Identify and select important features\n",
    "5. **End-to-End Pipeline** - Complete workflow from raw data to ML-ready\n",
    "6. **Advanced Techniques** - Statistical robustness and production patterns\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import all necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\n\nfrom feature_engineering_tk import (\n    DataAnalyzer,\n    TargetAnalyzer,\n    DataPreprocessor,\n    FeatureEngineer,\n    FeatureSelector,\n    statistical_utils\n)\nfrom feature_engineering_tk.data_analysis import quick_analysis\nfrom feature_engineering_tk.feature_selection import select_features_auto\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Configure plotting\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette('husl')\n\nprint(\"Setup complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Exploratory Data Analysis\n",
    "\n",
    "We'll analyze a real estate price prediction dataset to understand the data before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Real Estate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate realistic real estate data\n",
    "n_houses = 5000\n",
    "\n",
    "df_houses = pd.DataFrame({\n",
    "    'square_feet': np.random.normal(2000, 800, n_houses).clip(500, 10000),\n",
    "    'bedrooms': np.random.choice([1, 2, 3, 4, 5, 6], n_houses, p=[0.05, 0.15, 0.35, 0.30, 0.10, 0.05]),\n",
    "    'bathrooms': np.random.choice([1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0], n_houses),\n",
    "    'age_years': np.random.exponential(15, n_houses).clip(0, 100),\n",
    "    'lot_size': np.random.lognormal(8.5, 0.5, n_houses),\n",
    "    'garage_spaces': np.random.choice([0, 1, 2, 3], n_houses, p=[0.1, 0.2, 0.5, 0.2]),\n",
    "    'neighborhood': np.random.choice(['Downtown', 'Suburb', 'Rural', 'Waterfront'], n_houses, p=[0.2, 0.5, 0.2, 0.1]),\n",
    "    'property_type': np.random.choice(['Single Family', 'Condo', 'Townhouse'], n_houses, p=[0.6, 0.25, 0.15]),\n",
    "    'has_pool': np.random.choice([0, 1], n_houses, p=[0.85, 0.15]),\n",
    "    'has_fireplace': np.random.choice([0, 1], n_houses, p=[0.7, 0.3]),\n",
    "    'school_rating': np.random.choice(range(1, 11), n_houses),\n",
    "})\n",
    "\n",
    "# Create target with realistic price model\n",
    "base_price = 100000\n",
    "price = (\n",
    "    base_price +\n",
    "    df_houses['square_feet'] * 150 +\n",
    "    df_houses['bedrooms'] * 20000 +\n",
    "    df_houses['bathrooms'] * 15000 +\n",
    "    df_houses['garage_spaces'] * 10000 -\n",
    "    df_houses['age_years'] * 1000 +\n",
    "    df_houses['lot_size'] * 5 +\n",
    "    df_houses['has_pool'] * 25000 +\n",
    "    df_houses['has_fireplace'] * 8000 +\n",
    "    df_houses['school_rating'] * 5000 +\n",
    "    np.where(df_houses['neighborhood'] == 'Waterfront', 100000, 0) +\n",
    "    np.where(df_houses['neighborhood'] == 'Downtown', 50000, 0) +\n",
    "    np.random.normal(0, 30000, n_houses)  # Random variation\n",
    ")\n",
    "df_houses['price'] = price.clip(50000, 2000000)\n",
    "\n",
    "# Add missing values\n",
    "missing_idx = np.random.choice(df_houses.index, size=int(0.08 * n_houses), replace=False)\n",
    "df_houses.loc[missing_idx, 'lot_size'] = np.nan\n",
    "\n",
    "missing_idx2 = np.random.choice(df_houses.index, size=int(0.05 * n_houses), replace=False)\n",
    "df_houses.loc[missing_idx2, 'school_rating'] = np.nan\n",
    "\n",
    "print(f\"Dataset shape: {df_houses.shape}\")\n",
    "df_houses.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataAnalyzer: General EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview\n",
    "quick_analysis(df_houses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = DataAnalyzer(df_houses)\n",
    "\n",
    "# Basic information\n",
    "basic_info = analyzer.get_basic_info()\n",
    "print(basic_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value analysis\n",
    "missing_summary = analyzer.get_missing_summary()\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric summary statistics\n",
    "numeric_summary = analyzer.get_numeric_summary()\n",
    "print(numeric_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detect outliers using IQR method\noutlier_summary_iqr = analyzer.detect_outliers_iqr()\nprint(\"Outliers detected (IQR method):\")\nprint(outlier_summary_iqr)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlations\n",
    "high_correlations = analyzer.get_high_correlations(threshold=0.6)\n",
    "if not high_correlations.empty:\n",
    "    print(\"High correlations found:\")\n",
    "    print(high_correlations)\n",
    "else:\n",
    "    print(\"No high correlations above 0.6 threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multicollinearity using VIF\n",
    "numeric_cols = df_houses.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(numeric_cols) > 1:\n",
    "    vif_results = analyzer.calculate_vif(columns=numeric_cols)\n",
    "    print(\"\\nVariance Inflation Factor (VIF):\")\n",
    "    print(vif_results)\n",
    "    print(\"\\nNote: VIF > 10 indicates high multicollinearity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detect misclassified categorical columns\nmisclassified = analyzer.detect_misclassified_categorical()\nif not misclassified.empty:\n    print(\"Columns that should be categorical:\")\n    for idx, row in misclassified.iterrows():\n        print(f\"  {row['column']}: {row['reason']} (unique values: {row['unique_count']})\")\nelse:\n    print(\"No misclassified categorical columns detected.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get binning suggestions\nbinning_suggestions = analyzer.suggest_binning()\nif not binning_suggestions.empty:\n    print(\"Binning suggestions:\")\n    for idx, row in binning_suggestions.iterrows():\n        print(f\"\\n{row['column']}:\")\n        print(f\"  Strategy: {row['strategy']}\")\n        print(f\"  Bins: {row['n_bins']}\")\n        print(f\"  Reason: {row['reason']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation heatmap\n",
    "fig = analyzer.plot_correlation_heatmap(method='pearson', show=False)\n",
    "if fig:\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TargetAnalyzer: Target-Aware Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize target analyzer\n",
    "target_analyzer = TargetAnalyzer(df_houses, target_column='price')\n",
    "\n",
    "# Auto-detect task type\n",
    "print(f\"Task type: {target_analyzer.task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target distribution\n",
    "target_stats = target_analyzer.analyze_target_distribution()\n",
    "print(\"Target Distribution Statistics:\")\n",
    "for key, value in target_stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target distribution\n",
    "fig = target_analyzer.plot_target_distribution(show=False)\n",
    "if fig:\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature correlations with target\n",
    "correlations = target_analyzer.analyze_feature_correlations(method='pearson')\n",
    "print(\"Top 10 Features by Correlation with Price:\")\n",
    "print(correlations.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze mutual information scores\n",
    "mi_scores = target_analyzer.analyze_mutual_information()\n",
    "print(\"\\nTop 10 Features by Mutual Information:\")\n",
    "print(mi_scores.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality\n",
    "quality_report = target_analyzer.analyze_data_quality()\n",
    "print(\"Data Quality Report:\")\n",
    "print(f\"Missing values: {quality_report['missing_values_count']}\")\n",
    "print(f\"Constant features: {quality_report['constant_features_count']}\")\n",
    "if quality_report['constant_features']:\n",
    "    print(f\"  Constant feature list: {quality_report['constant_features']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature engineering suggestions\n",
    "fe_suggestions = target_analyzer.suggest_feature_engineering()\n",
    "print(\"\\nTop Feature Engineering Suggestions:\")\n",
    "high_priority = [s for s in fe_suggestions if s['priority'] == 'high']\n",
    "for i, sugg in enumerate(high_priority[:8], 1):\n",
    "    print(f\"\\n{i}. {sugg['feature']} - {sugg['suggestion']}\")\n",
    "    print(f\"   Reason: {sugg['reason']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model recommendations\n",
    "model_recs = target_analyzer.recommend_models()\n",
    "print(\"\\nModel Recommendations:\")\n",
    "for i, rec in enumerate(model_recs[:5], 1):\n",
    "    print(f\"\\n{i}. {rec['model']} (Priority: {rec['priority']})\")\n",
    "    print(f\"   Why: {rec['reason']}\")\n",
    "    print(f\"   Note: {rec['considerations']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "report = target_analyzer.generate_full_report()\n",
    "print(\"\\nComprehensive Report Generated:\")\n",
    "print(f\"Sections: {list(report.keys())}\")\n",
    "\n",
    "# Export to HTML\n",
    "target_analyzer.export_report('real_estate_analysis.html', format='html')\n",
    "print(\"\\nReport exported to real_estate_analysis.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Data Preprocessing\n",
    "\n",
    "Let's work with a messy customer dataset that has missing values, outliers, and text columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Messy Customer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate customer dataset with various data quality issues\n",
    "n_customers = 3000\n",
    "\n",
    "# Create base date\n",
    "base_date = datetime(2023, 1, 1)\n",
    "\n",
    "df_customers = pd.DataFrame({\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'name': [f\"Customer {i}\" for i in range(1, n_customers + 1)],\n",
    "    'email': [f\"user{i}@example.com\" if np.random.random() > 0.1 else f\"  USER{i}@EXAMPLE.COM  \" for i in range(1, n_customers + 1)],\n",
    "    'signup_date': [base_date + timedelta(days=int(x)) for x in np.random.uniform(0, 730, n_customers)],\n",
    "    'age': np.random.normal(45, 15, n_customers).clip(18, 90),\n",
    "    'income': np.random.lognormal(10.8, 0.6, n_customers),\n",
    "    'credit_score': np.random.normal(680, 80, n_customers).clip(300, 850),\n",
    "    'num_purchases': np.random.poisson(8, n_customers),\n",
    "    'total_spent': np.random.exponential(500, n_customers),\n",
    "    'membership_tier': np.random.choice(['  Bronze  ', 'Silver', 'GOLD', 'platinum'], n_customers, p=[0.5, 0.3, 0.15, 0.05]),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_customers),\n",
    "    'is_active': np.random.choice([0, 1], n_customers, p=[0.3, 0.7]),\n",
    "})\n",
    "\n",
    "# Add missing values (realistic patterns)\n",
    "missing_idx = np.random.choice(df_customers.index, size=int(0.15 * n_customers), replace=False)\n",
    "df_customers.loc[missing_idx, 'income'] = np.nan\n",
    "\n",
    "missing_idx2 = np.random.choice(df_customers.index, size=int(0.08 * n_customers), replace=False)\n",
    "df_customers.loc[missing_idx2, 'credit_score'] = np.nan\n",
    "\n",
    "missing_idx3 = np.random.choice(df_customers.index, size=int(0.05 * n_customers), replace=False)\n",
    "df_customers.loc[missing_idx3, 'total_spent'] = np.nan\n",
    "\n",
    "# Add outliers\n",
    "outlier_idx = np.random.choice(df_customers.index, size=30, replace=False)\n",
    "df_customers.loc[outlier_idx, 'total_spent'] = np.random.uniform(5000, 20000, 30)\n",
    "\n",
    "# Add infinite values\n",
    "inf_idx = np.random.choice(df_customers.index, size=5, replace=False)\n",
    "df_customers.loc[inf_idx, 'income'] = np.inf\n",
    "\n",
    "# Add duplicates\n",
    "df_customers = pd.concat([df_customers, df_customers.sample(10)], ignore_index=True)\n",
    "\n",
    "print(f\"Dataset shape: {df_customers.shape}\")\n",
    "print(f\"Missing values: {df_customers.isnull().sum().sum()}\")\n",
    "df_customers.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize preprocessor\npreprocessor = DataPreprocessor(df_customers)\n\n# Validate data quality\nquality_report = preprocessor.validate_data_quality()\nprint(\"Data Quality Report:\")\nprint(f\"Total rows: {quality_report['shape'][0]}\")\nprint(f\"Total columns: {quality_report['shape'][1]}\")\nprint(f\"Missing values: {quality_report['missing_values']}\")\nprint(f\"Duplicate rows: {quality_report['duplicate_rows']}\")\nprint(f\"Constant columns: {quality_report['constant_columns']}\")\nprint(f\"Issues found: {quality_report['issues_found']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Detect infinite values\ninfinite_vals = preprocessor.detect_infinite_values()\nif infinite_vals:\n    print(\"Infinite values detected:\")\n    for col, count in infinite_vals.items():\n        print(f\"  {col}: {count} infinite values\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values with appropriate strategies\n",
    "# For normally distributed numeric columns: mean\n",
    "preprocessor.handle_missing_values(\n",
    "    columns=['age', 'credit_score'],\n",
    "    strategy='mean',\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# For skewed numeric columns: median\n",
    "preprocessor.handle_missing_values(\n",
    "    columns=['income', 'total_spent'],\n",
    "    strategy='median',\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# For categorical columns: mode\n",
    "preprocessor.handle_missing_values(\n",
    "    columns=['membership_tier'],\n",
    "    strategy='mode',\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "df_processed = preprocessor.get_dataframe()\n",
    "print(f\"Missing values after imputation: {df_processed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean string columns\n",
    "preprocessor.clean_string_columns(\n",
    "    columns=['email', 'membership_tier'],\n",
    "    operations=['strip', 'lower'],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Handle whitespace variants\n",
    "preprocessor.handle_whitespace_variants(\n",
    "    columns=['membership_tier'],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Extract string length as a feature\n",
    "preprocessor.extract_string_length(\n",
    "    columns=['name'],\n",
    "    suffix='_length',\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "df_processed = preprocessor.get_dataframe()\n",
    "print(\"\\nCleaned membership tiers:\")\n",
    "print(df_processed['membership_tier'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers and Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "rows_before = df_processed.shape[0]\n",
    "preprocessor.remove_duplicates(inplace=True)\n",
    "df_processed = preprocessor.get_dataframe()\n",
    "print(f\"Duplicates removed: {rows_before - df_processed.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers in spending (cap extreme values)\n",
    "preprocessor.handle_outliers(\n",
    "    columns=['total_spent'],\n",
    "    method='iqr',\n",
    "    action='cap',\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "print(\"Outliers capped in total_spent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinite values\n",
    "preprocessor.clip_values(\n",
    "    columns=['income'],\n",
    "    lower=0,\n",
    "    upper=1000000,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "df_processed = preprocessor.get_dataframe()\n",
    "print(f\"Final dataset shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation History Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View preprocessing history\n",
    "summary = preprocessor.get_preprocessing_summary()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export preprocessing report\n",
    "preprocessor.export_summary('customer_preprocessing.md', format='markdown')\n",
    "print(\"Preprocessing report exported to customer_preprocessing.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Feature Engineering\n",
    "\n",
    "Let's work with e-commerce transaction data to create powerful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate E-Commerce Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate e-commerce transaction data\n",
    "n_transactions = 8000\n",
    "n_customers_ecom = 1000\n",
    "\n",
    "base_date = datetime(2023, 1, 1)\n",
    "\n",
    "df_ecommerce = pd.DataFrame({\n",
    "    'transaction_id': range(1, n_transactions + 1),\n",
    "    'customer_id': np.random.randint(1, n_customers_ecom + 1, n_transactions),\n",
    "    'transaction_date': [base_date + timedelta(days=int(x)) for x in np.random.uniform(0, 365, n_transactions)],\n",
    "    'order_value': np.random.lognormal(4, 1, n_transactions),\n",
    "    'item_count': np.random.poisson(3, n_transactions) + 1,\n",
    "    'discount_amount': np.random.exponential(5, n_transactions),\n",
    "    'shipping_cost': np.random.uniform(0, 25, n_transactions),\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Home', 'Books', 'Sports'], n_transactions),\n",
    "    'payment_method': np.random.choice(['Credit Card', 'Debit Card', 'PayPal', 'Gift Card'], n_transactions),\n",
    "    'device_type': np.random.choice(['Desktop', 'Mobile', 'Tablet'], n_transactions, p=[0.4, 0.5, 0.1]),\n",
    "    'is_weekend': np.random.choice([0, 1], n_transactions, p=[0.7, 0.3]),\n",
    "    'is_repeat_customer': np.random.choice([0, 1], n_transactions, p=[0.4, 0.6]),\n",
    "})\n",
    "\n",
    "# Calculate final amount\n",
    "df_ecommerce['final_amount'] = (\n",
    "    df_ecommerce['order_value'] -\n",
    "    df_ecommerce['discount_amount'] +\n",
    "    df_ecommerce['shipping_cost']\n",
    ").clip(lower=1)\n",
    "\n",
    "print(f\"E-commerce dataset shape: {df_ecommerce.shape}\")\n",
    "df_ecommerce.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize feature engineer\nengineer = FeatureEngineer(df_ecommerce)\n\n# One-hot encoding for nominal categories\nengineer.encode_categorical_onehot(\n    columns=['category', 'payment_method', 'device_type'],\n    inplace=True\n)\n\ndf_engineered = engineer.get_dataframe()\n\nprint(f\"Shape after one-hot encoding: {df_engineered.shape}\")\nprint(f\"New columns created: {df_engineered.shape[1] - df_ecommerce.shape[1]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Standard scaling for normally distributed features\nengineer.scale_features(\n    columns=['item_count'],\n    method='standard',\n    inplace=True\n)\n\n# Robust scaling for features with outliers\nengineer.scale_features(\n    columns=['order_value', 'discount_amount', 'shipping_cost', 'final_amount'],\n    method='robust',\n    inplace=True\n)\n\nprint(\"Scaling complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Log transform for skewed distributions\nengineer.create_log_transform(\n    columns=['order_value'],\n    inplace=True\n)\n\n# Polynomial features for non-linear relationships\nengineer.create_polynomial_features(\n    columns=['item_count'],\n    degree=2,\n    inplace=True\n)\n\ndf_engineered = engineer.get_dataframe()\nprint(f\"Shape after transformations: {df_engineered.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract datetime components\nengineer.create_datetime_features(\n    column='transaction_date',\n    features=['year', 'month', 'day', 'dayofweek', 'quarter'],\n    inplace=True\n)\n\ndf_engineered = engineer.get_dataframe()\n\nprint(\"Datetime features extracted:\")\ndatetime_cols = [col for col in df_engineered.columns if col.startswith('transaction_date_')]\nprint(datetime_cols)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ratio features\nengineer.create_ratio_features(\n    numerator='discount_amount',\n    denominator='order_value',\n    name='discount_rate',\n    inplace=True\n)\n\nengineer.create_ratio_features(\n    numerator='shipping_cost',\n    denominator='order_value',\n    name='shipping_rate',\n    inplace=True\n)\n\n# Flag features\nengineer.create_flag_features(\n    column='discount_amount',\n    condition=lambda x: x > 10,\n    flag_name='has_large_discount',\n    inplace=True\n)\n\nengineer.create_flag_features(\n    column='order_value',\n    condition=lambda x: x > 10,\n    flag_name='is_high_value',\n    inplace=True\n)\n\ndf_engineered = engineer.get_dataframe()\n\nprint(f\"\\nFinal engineered dataset shape: {df_engineered.shape}\")\nprint(f\"Total new features created: {df_engineered.shape[1] - df_ecommerce.shape[1]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Features by Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Aggregate spending by customer\nengineer.create_aggregations(\n    agg_column='order_value',\n    group_by='customer_id',\n    agg_funcs=['mean', 'sum', 'count'],\n    inplace=True\n)\n\ndf_engineered = engineer.get_dataframe()\n\nprint(\"Aggregation features created:\")\nagg_cols = [col for col in df_engineered.columns if 'order_value_customer_id' in col]\nprint(agg_cols)\nprint(f\"\\nFinal shape: {df_engineered.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Transformers for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fitted transformers\n",
    "engineer.save_transformers('ecommerce_transformers.pkl')\n",
    "print(\"Transformers saved to ecommerce_transformers.pkl\")\n",
    "print(f\"Saved transformers: {list(engineer.encoders.keys()) + list(engineer.scalers.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate loading transformers (for new data)\n",
    "new_engineer = FeatureEngineer(df_ecommerce.head(100))  # Simulate new data\n",
    "new_engineer.load_transformers('ecommerce_transformers.pkl')\n",
    "print(f\"\\nTransformers loaded successfully!\")\n",
    "print(f\"Loaded encoders: {list(new_engineer.encoders.keys())}\")\n",
    "print(f\"Loaded scalers: {list(new_engineer.scalers.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Feature Selection\n",
    "\n",
    "We'll work with a high-dimensional dataset and select the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate High-Dimensional Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset with many features\n",
    "n_samples = 2000\n",
    "n_features = 50\n",
    "\n",
    "# Create features\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "feature_names = [f'feature_{i}' for i in range(1, n_features + 1)]\n",
    "\n",
    "# Create target with only some features being truly predictive\n",
    "important_features = [0, 5, 10, 15, 20, 25, 30, 35, 40]\n",
    "y = (\n",
    "    X[:, 0] * 2 +\n",
    "    X[:, 5] * 1.5 +\n",
    "    X[:, 10] * 1.2 +\n",
    "    X[:, 15] * 1.0 +\n",
    "    X[:, 20] * 0.8 +\n",
    "    X[:, 25] * 0.6 +\n",
    "    X[:, 30] * 0.5 +\n",
    "    X[:, 35] * 0.4 +\n",
    "    X[:, 40] * 0.3 +\n",
    "    np.random.randn(n_samples) * 0.5  # Add noise\n",
    ")\n",
    "\n",
    "# Add some constant and near-constant features\n",
    "X[:, -1] = 1  # Constant\n",
    "X[:, -2] = np.random.choice([0, 1], n_samples, p=[0.99, 0.01])  # Near constant\n",
    "\n",
    "# Add highly correlated features\n",
    "X[:, -3] = X[:, 0] + np.random.randn(n_samples) * 0.01\n",
    "X[:, -4] = X[:, 5] + np.random.randn(n_samples) * 0.01\n",
    "\n",
    "df_highdim = pd.DataFrame(X, columns=feature_names)\n",
    "df_highdim['target'] = y\n",
    "\n",
    "print(f\"High-dimensional dataset shape: {df_highdim.shape}\")\n",
    "print(f\"Target statistics: mean={y.mean():.2f}, std={y.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize feature selector\nselector = FeatureSelector(df_highdim, target_column='target')\n\n# Remove low variance features\nvariance_features = selector.select_by_variance(threshold=0.01)\ndf_variance = selector.apply_selection(variance_features, keep_target=True)\n\nprint(f\"Features after variance filter: {df_variance.shape[1] - 1}\")\nprint(f\"Features removed: {df_highdim.shape[1] - df_variance.shape[1]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Remove highly correlated features\nselector_corr = FeatureSelector(df_variance, target_column='target')\ncorr_features = selector_corr.select_by_correlation(threshold=0.95)\ndf_corr = selector_corr.apply_selection(corr_features, keep_target=True)\n\nprint(f\"\\nFeatures after correlation filter: {df_corr.shape[1] - 1}\")\nprint(f\"Features removed: {df_variance.shape[1] - df_corr.shape[1]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select by target correlation\nselector_target = FeatureSelector(df_corr, target_column='target')\ntarget_corr_features = selector_target.select_by_target_correlation(k=20)\ntarget_corr_df = selector_target.apply_selection(target_corr_features, keep_target=True)\n\nprint(f\"\\nTop 20 features with highest target correlation: {target_corr_df.shape[1] - 1}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Statistical test selection\nselector_stat = FeatureSelector(df_corr, target_column='target')\ndf_ftest = selector_stat.select_by_statistical_test(score_func='f_regression', k=20)\nprint(f\"\\nTop 20 features by F-test: {df_ftest.shape[1] - 1}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mutual information selection\ndf_mi = selector_stat.select_by_statistical_test(score_func='mutual_info_regression', k=15)\nprint(f\"Top 15 features by mutual information: {df_mi.shape[1] - 1}\")\nprint(f\"\\nSelected features: {[col for col in df_mi.columns if col != 'target'][:10]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Feature Selection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use automatic 3-step selection\ndf_auto_selected = select_features_auto(\n    df=df_highdim,\n    target_column='target',\n    task='regression',\n    max_features=15,\n    variance_threshold=0.01,\n    correlation_threshold=0.95\n)\n\nprint(f\"Original features: {df_highdim.shape[1] - 1}\")\nprint(f\"Selected features: {df_auto_selected.shape[1] - 1}\")\nprint(f\"Reduction: {(1 - (df_auto_selected.shape[1] - 1) / (df_highdim.shape[1] - 1)) * 100:.1f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get feature importance scores (stored after selection)\nselector_auto = FeatureSelector(df_highdim, target_column='target')\nif selector_auto.feature_scores is not None:\n    print(\"\\nTop 10 Features by Importance:\")\n    print(selector_auto.feature_scores.head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-Based Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select features using tree-based importance\nselector_tree = FeatureSelector(df_highdim, target_column='target')\ndf_tree_selected = selector_tree.select_by_importance(\n    k=15,\n    task='regression'\n)\n\nprint(f\"Features selected by Random Forest importance: {df_tree_selected.shape[1] - 1}\")\nprint(f\"\\nSelected features:\")\nselected_features_tree = [col for col in df_tree_selected.columns if col != 'target']\nfor i, feat in enumerate(selected_features_tree, 1):\n    print(f\"{i}. {feat}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Complete End-to-End Pipeline\n",
    "\n",
    "Let's put everything together with an insurance claim prediction example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Insurance Claims Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insurance claims data\n",
    "n_claims = 4000\n",
    "\n",
    "df_insurance = pd.DataFrame({\n",
    "    'policy_id': range(1, n_claims + 1),\n",
    "    'age': np.random.randint(18, 80, n_claims),\n",
    "    'gender': np.random.choice(['Male', 'Female'], n_claims),\n",
    "    'bmi': np.random.normal(28, 6, n_claims).clip(15, 50),\n",
    "    'children': np.random.choice([0, 1, 2, 3, 4, 5], n_claims, p=[0.4, 0.25, 0.2, 0.1, 0.04, 0.01]),\n",
    "    'smoker': np.random.choice(['Yes', 'No'], n_claims, p=[0.2, 0.8]),\n",
    "    'region': np.random.choice(['Northeast', 'Northwest', 'Southeast', 'Southwest'], n_claims),\n",
    "    'coverage_type': np.random.choice(['Basic', 'Standard', 'Premium'], n_claims, p=[0.3, 0.5, 0.2]),\n",
    "    'years_insured': np.random.randint(0, 40, n_claims),\n",
    "    'previous_claims': np.random.poisson(1.5, n_claims),\n",
    "    'vehicle_age': np.random.randint(0, 20, n_claims),\n",
    "})\n",
    "\n",
    "# Create target (claim amount)\n",
    "base_claim = 5000\n",
    "claim = (\n",
    "    base_claim +\n",
    "    df_insurance['age'] * 100 +\n",
    "    df_insurance['bmi'] * 200 +\n",
    "    df_insurance['children'] * 1000 +\n",
    "    np.where(df_insurance['smoker'] == 'Yes', 10000, 0) +\n",
    "    df_insurance['previous_claims'] * 3000 +\n",
    "    df_insurance['vehicle_age'] * 500 +\n",
    "    np.where(df_insurance['coverage_type'] == 'Premium', 5000, 0) +\n",
    "    np.where(df_insurance['coverage_type'] == 'Standard', 2000, 0) +\n",
    "    np.random.normal(0, 2000, n_claims)\n",
    ")\n",
    "df_insurance['claim_amount'] = claim.clip(1000, 100000)\n",
    "\n",
    "# Add realistic data issues\n",
    "missing_idx = np.random.choice(df_insurance.index, size=int(0.06 * n_claims), replace=False)\n",
    "df_insurance.loc[missing_idx, 'bmi'] = np.nan\n",
    "\n",
    "missing_idx2 = np.random.choice(df_insurance.index, size=int(0.04 * n_claims), replace=False)\n",
    "df_insurance.loc[missing_idx2, 'vehicle_age'] = np.nan\n",
    "\n",
    "# Add outliers\n",
    "outlier_idx = np.random.choice(df_insurance.index, size=20, replace=False)\n",
    "df_insurance.loc[outlier_idx, 'claim_amount'] = np.random.uniform(150000, 250000, 20)\n",
    "\n",
    "print(f\"Insurance dataset shape: {df_insurance.shape}\")\n",
    "df_insurance.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick analysis\n",
    "quick_analysis(df_insurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target analysis\n",
    "target_analyzer_insurance = TargetAnalyzer(df_insurance, target_column='claim_amount')\n",
    "\n",
    "target_dist = target_analyzer_insurance.analyze_target_distribution()\n",
    "print(\"\\nClaim Amount Statistics:\")\n",
    "print(f\"Mean: ${target_dist['mean']:.2f}\")\n",
    "print(f\"Median: ${target_dist['median']:.2f}\")\n",
    "print(f\"Std: ${target_dist['std']:.2f}\")\n",
    "print(f\"Skewness: {target_dist['skewness']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlations\n",
    "correlations_insurance = target_analyzer_insurance.analyze_feature_correlations()\n",
    "print(\"\\nTop Features by Correlation:\")\n",
    "print(correlations_insurance.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor and clean data\n",
    "preprocessor_insurance = DataPreprocessor(df_insurance)\n",
    "\n",
    "preprocessor_insurance\\\n",
    "    .drop_columns(['policy_id'], inplace=True)\\\n",
    "    .handle_missing_values(strategy='median', columns=['bmi', 'vehicle_age'], inplace=True)\\\n",
    "    .handle_outliers(columns=['claim_amount'], method='iqr', action='cap', inplace=True)\n",
    "\n",
    "df_insurance_clean = preprocessor_insurance.get_dataframe()\n",
    "print(f\"Cleaned dataset shape: {df_insurance_clean.shape}\")\n",
    "print(f\"Missing values: {df_insurance_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize feature engineer\nengineer_insurance = FeatureEngineer(df_insurance_clean)\n\n# Encode categorical variables\nengineer_insurance.encode_categorical_onehot(\n    columns=['gender', 'smoker', 'region', 'coverage_type'],\n    inplace=True\n)\n\n# Create polynomial features\nengineer_insurance.create_polynomial_features(\n    columns=['bmi'],\n    degree=2,\n    inplace=True\n)\n\n# Create interaction features (age * bmi)\nengineer_insurance.create_polynomial_features(\n    columns=['age', 'bmi'],\n    degree=2,\n    interaction_only=True,\n    inplace=True\n)\n\n# Create flags\nengineer_insurance.create_flag_features(\n    column='bmi',\n    condition=lambda x: x > 30,\n    flag_name='is_obese',\n    inplace=True\n)\n\nengineer_insurance.create_flag_features(\n    column='previous_claims',\n    condition=lambda x: x > 2,\n    flag_name='has_frequent_claims',\n    inplace=True\n)\n\n# Scale numeric features\nnumeric_features = ['age', 'bmi', 'children', 'years_insured', 'previous_claims', 'vehicle_age']\nengineer_insurance.scale_features(\n    columns=numeric_features,\n    method='standard',\n    inplace=True\n)\n\ndf_insurance_eng = engineer_insurance.get_dataframe()\n\nprint(f\"Engineered dataset shape: {df_insurance_eng.shape}\")\nprint(f\"Features created: {df_insurance_eng.shape[1] - df_insurance_clean.shape[1]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select best features using automatic pipeline\ndf_insurance_final = select_features_auto(\n    df=df_insurance_eng,\n    target_column='claim_amount',\n    task='regression',\n    max_features=20,\n    variance_threshold=0.01,\n    correlation_threshold=0.95\n)\n\nprint(f\"\\nOriginal features: {df_insurance_eng.shape[1] - 1}\")\nprint(f\"Selected features: {df_insurance_final.shape[1] - 1}\")\nprint(f\"\\nSelected feature list:\")\nfor i, feat in enumerate([col for col in df_insurance_final.columns if col != 'claim_amount'], 1):\n    print(f\"{i}. {feat}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Final Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze with final features\n",
    "final_analyzer = TargetAnalyzer(df_insurance_final, target_column='claim_amount')\n",
    "\n",
    "# Get feature engineering suggestions\n",
    "fe_suggestions_insurance = final_analyzer.suggest_feature_engineering()\n",
    "print(\"Additional Feature Engineering Suggestions:\")\n",
    "for i, sugg in enumerate(fe_suggestions_insurance[:5], 1):\n",
    "    print(f\"\\n{i}. {sugg['feature']}\")\n",
    "    print(f\"   {sugg['suggestion']} (Priority: {sugg['priority']})\")\n",
    "    print(f\"   Reason: {sugg['reason']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model recommendations\n",
    "model_recs_insurance = final_analyzer.recommend_models()\n",
    "print(\"\\nModel Recommendations:\")\n",
    "for i, rec in enumerate(model_recs_insurance[:4], 1):\n",
    "    print(f\"\\n{i}. {rec['model']} (Priority: {rec['priority']})\")\n",
    "    print(f\"   Reason: {rec['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Export Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export preprocessing report\n",
    "preprocessor_insurance.export_summary('insurance_preprocessing.json', format='json')\n",
    "print(\"Preprocessing report exported\")\n",
    "\n",
    "# Export analysis report\n",
    "final_analyzer.export_report('insurance_analysis.html', format='html')\n",
    "print(\"Analysis report exported\")\n",
    "\n",
    "# Save transformers for production\n",
    "engineer_insurance.save_transformers('insurance_transformers.pkl')\n",
    "print(\"Transformers saved for production deployment\")\n",
    "\n",
    "# Save final dataset\n",
    "df_insurance_final.to_csv('insurance_ml_ready.csv', index=False)\n",
    "print(\"ML-ready dataset saved\")\n",
    "\n",
    "print(\"\\n✓ End-to-end pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Apply Pipeline to New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate new test data\n",
    "df_new = df_insurance.sample(100, random_state=99).copy()\n",
    "print(f\"New data shape: {df_new.shape}\")\n",
    "\n",
    "# Apply same preprocessing steps\n",
    "prep_new = DataPreprocessor(df_new)\n",
    "prep_new\\\n",
    "    .drop_columns(['policy_id'], inplace=True)\\\n",
    "    .handle_missing_values(strategy='median', columns=['bmi', 'vehicle_age'], inplace=True)\\\n",
    "    .handle_outliers(columns=['claim_amount'], method='iqr', action='cap', inplace=True)\n",
    "\n",
    "df_new_clean = prep_new.get_dataframe()\n",
    "\n",
    "# Load transformers and apply\n",
    "eng_new = FeatureEngineer(df_new_clean)\n",
    "eng_new.load_transformers('insurance_transformers.pkl')\n",
    "\n",
    "# Apply same transformations\n",
    "df_new_transformed = eng_new.encode_categorical_onehot(\n",
    "    columns=['gender', 'smoker', 'region', 'coverage_type'],\n",
    "    method='onehot'\n",
    ")\n",
    "\n",
    "print(f\"\\nNew data after transformation: {df_new_transformed.shape}\")\n",
    "print(\"Pipeline successfully applied to new data!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 6: Advanced Techniques\n",
    "\n",
    "Let's explore statistical robustness and production patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Robustness with statistical_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data for statistical tests\n",
    "group1 = np.random.normal(100, 15, 200)\n",
    "group2 = np.random.normal(105, 15, 200)\n",
    "group3 = np.random.normal(95, 15, 200)\n",
    "\n",
    "# Check normality assumption\n",
    "normality_check = statistical_utils.check_normality(group1)\n",
    "print(\"Normality Check:\")\n",
    "print(f\"Is normal: {normality_check['is_normal']}\")\n",
    "print(f\"P-value: {normality_check['pvalue']:.4f}\")\n",
    "print(f\"Recommendation: {normality_check['recommendation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check homogeneity of variance\n",
    "homogeneity_check = statistical_utils.check_homogeneity_of_variance([group1, group2, group3])\n",
    "print(\"\\nHomogeneity of Variance:\")\n",
    "print(f\"Equal variances: {homogeneity_check['equal_variances']}\")\n",
    "print(f\"P-value: {homogeneity_check['pvalue']:.4f}\")\n",
    "print(f\"Recommendation: {homogeneity_check['recommendation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate effect size (Cohen's d)\n",
    "effect_size = statistical_utils.cohens_d(group1, group2)\n",
    "print(\"\\nEffect Size (Cohen's d):\")\n",
    "print(f\"Cohen's d: {effect_size['cohens_d']:.3f}\")\n",
    "print(f\"Interpretation: {effect_size['interpretation']}\")\n",
    "print(f\"Description: {effect_size['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence interval for mean\n",
    "ci = statistical_utils.calculate_mean_ci(group1, confidence=0.95)\n",
    "print(\"\\nConfidence Interval for Mean:\")\n",
    "print(f\"Mean: {ci['mean']:.2f}\")\n",
    "print(f\"95% CI: [{ci['ci_lower']:.2f}, {ci['ci_upper']:.2f}]\")\n",
    "print(f\"Margin of error: ±{ci['margin_of_error']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple testing correction\n",
    "pvalues = np.array([0.001, 0.01, 0.03, 0.04, 0.05, 0.06, 0.08, 0.10, 0.20, 0.50])\n",
    "correction = statistical_utils.apply_multiple_testing_correction(pvalues, method='fdr_bh', alpha=0.05)\n",
    "\n",
    "print(\"\\nMultiple Testing Correction (FDR):\")\n",
    "print(f\"Significant before correction: {correction['num_significant_raw']}\")\n",
    "print(f\"Significant after correction: {correction['num_significant_corrected']}\")\n",
    "print(f\"\\nRejected null hypotheses: {correction['reject']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap confidence interval\n",
    "bootstrap_ci = statistical_utils.bootstrap_ci(\n",
    "    group1,\n",
    "    statistic_func=np.median,\n",
    "    n_bootstrap=1000,\n",
    "    confidence=0.95\n",
    ")\n",
    "\n",
    "print(\"\\nBootstrap CI for Median:\")\n",
    "print(f\"Median: {bootstrap_ci['statistic']:.2f}\")\n",
    "print(f\"95% CI: [{bootstrap_ci['ci_lower']:.2f}, {bootstrap_ci['ci_upper']:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Patterns and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Creating a reproducible preprocessing pipeline class\nclass InsurancePreprocessingPipeline:\n    def __init__(self):\n        self.preprocessor = None\n        self.engineer = None\n        self.selected_features = None\n        \n    def fit(self, df_train, target_col='claim_amount'):\n        \"\"\"Fit the pipeline on training data.\"\"\"\n        # Preprocessing\n        self.preprocessor = DataPreprocessor(df_train)\n        self.preprocessor\\\n            .drop_columns(['policy_id'], inplace=True)\\\n            .handle_missing_values(strategy='median', columns=['bmi', 'vehicle_age'], inplace=True)\\\n            .handle_outliers(columns=[target_col], method='iqr', action='cap', inplace=True)\n        \n        df_clean = self.preprocessor.get_dataframe()\n        \n        # Feature engineering\n        self.engineer = FeatureEngineer(df_clean)\n        self.engineer.encode_categorical_onehot(\n            columns=['gender', 'smoker', 'region', 'coverage_type'],\n            method='onehot',\n            inplace=True\n        )\n        \n        df_eng = self.engineer.get_dataframe()\n        \n        # Feature selection\n        df_final = select_features_auto(\n            df=df_eng,\n            target_column=target_col,\n            task='regression',\n            max_features=20,\n            variance_threshold=0.01,\n            correlation_threshold=0.95\n        )\n        \n        # Store selected feature names\n        self.selected_features = [col for col in df_final.columns if col != target_col]\n        \n        return df_final\n    \n    def transform(self, df_test):\n        \"\"\"Transform test data using fitted pipeline.\"\"\"\n        # Apply same preprocessing steps\n        prep = DataPreprocessor(df_test)\n        prep\\\n            .drop_columns(['policy_id'], inplace=True)\\\n            .handle_missing_values(strategy='median', columns=['bmi', 'vehicle_age'], inplace=True)\n        \n        df_clean = prep.get_dataframe()\n        \n        # Apply fitted transformers\n        eng = FeatureEngineer(df_clean)\n        eng.encoders = self.engineer.encoders\n        eng.scalers = self.engineer.scalers\n        \n        eng.encode_categorical_onehot(\n            columns=['gender', 'smoker', 'region', 'coverage_type'],\n            method='onehot',\n            inplace=True\n        )\n        \n        df_eng = eng.get_dataframe()\n        \n        # Select same features\n        selected_cols = [col for col in self.selected_features if col in df_eng.columns]\n        df_final = df_eng[selected_cols + ['claim_amount']]\n        \n        return df_final\n\n# Example usage\npipeline = InsurancePreprocessingPipeline()\ndf_train_final = pipeline.fit(df_insurance.iloc[:3000])\ndf_test_final = pipeline.transform(df_insurance.iloc[3000:])\n\nprint(f\"Train shape after pipeline: {df_train_final.shape}\")\nprint(f\"Test shape after pipeline: {df_test_final.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Case Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 1: Handling constant columns\ndf_with_constant = pd.DataFrame({\n    'feature1': [1, 2, 3, 4, 5],\n    'feature2': [1, 1, 1, 1, 1],  # Constant\n    'target': [10, 20, 15, 25, 18]\n})\n\nanalyzer_const = DataAnalyzer(df_with_constant)\noutliers = analyzer_const.detect_outliers_zscore()\nprint(\"Edge Case 1: Constant Column Handling\")\nprint(\"Outliers detected (skips constant columns):\")\nprint(outliers)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Single-class target\n",
    "df_single_class = pd.DataFrame({\n",
    "    'feature1': np.random.randn(100),\n",
    "    'feature2': np.random.randn(100),\n",
    "    'target': [1] * 100  # All same class\n",
    "})\n",
    "\n",
    "try:\n",
    "    target_analyzer_single = TargetAnalyzer(df_single_class, target_column='target')\n",
    "    imbalance = target_analyzer_single.get_class_imbalance_info()\n",
    "    print(\"\\nEdge Case 2: Single-Class Target Handling\")\n",
    "    print(f\"Imbalance ratio: {imbalance['imbalance_ratio']}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nEdge Case 2: Handled gracefully - {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Highly imbalanced data\n",
    "df_imbalanced = pd.DataFrame({\n",
    "    'feature1': np.random.randn(1000),\n",
    "    'feature2': np.random.randn(1000),\n",
    "    'target': [0] * 950 + [1] * 50  # 95-5 split\n",
    "})\n",
    "\n",
    "target_analyzer_imb = TargetAnalyzer(df_imbalanced, target_column='target')\n",
    "imbalance_info = target_analyzer_imb.get_class_imbalance_info()\n",
    "\n",
    "print(\"\\nEdge Case 3: Highly Imbalanced Data\")\n",
    "print(f\"Imbalance severity: {imbalance_info['imbalance_severity']}\")\n",
    "print(f\"Imbalance ratio: {imbalance_info['imbalance_ratio']:.2f}\")\n",
    "print(f\"Recommendation: Use SMOTE or class weights during modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Wrap-Up\n",
    "\n",
    "## What We've Covered\n",
    "\n",
    "### 1. Exploratory Data Analysis\n",
    "- DataAnalyzer for general EDA\n",
    "- TargetAnalyzer for target-aware analysis\n",
    "- Statistical assumption checking\n",
    "- Correlation and multicollinearity detection\n",
    "\n",
    "### 2. Data Preprocessing\n",
    "- Missing value handling strategies\n",
    "- Outlier detection and treatment\n",
    "- String preprocessing\n",
    "- Data validation\n",
    "- Operation history tracking\n",
    "\n",
    "### 3. Feature Engineering\n",
    "- Encoding categorical variables\n",
    "- Scaling methods\n",
    "- Mathematical transformations\n",
    "- Datetime feature extraction\n",
    "- Creating derived features\n",
    "- Saving and loading transformers\n",
    "\n",
    "### 4. Feature Selection\n",
    "- Variance threshold\n",
    "- Correlation-based filtering\n",
    "- Statistical tests\n",
    "- Tree-based importance\n",
    "- Automatic selection pipeline\n",
    "\n",
    "### 5. End-to-End Pipeline\n",
    "- Complete workflow from raw data to ML-ready\n",
    "- Reproducible preprocessing\n",
    "- Applying pipeline to new data\n",
    "- Exporting reports and transformers\n",
    "\n",
    "### 6. Advanced Techniques\n",
    "- Statistical robustness utilities\n",
    "- Production deployment patterns\n",
    "- Edge case handling\n",
    "- Custom pipeline creation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Apply these patterns to your own datasets\n",
    "- Explore the statistical utilities for rigorous analysis\n",
    "- Build production-ready pipelines\n",
    "- Check out the GitHub repo for updates: https://github.com/bluelion1999/feature_engineering_tk\n",
    "\n",
    "Happy feature engineering! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}